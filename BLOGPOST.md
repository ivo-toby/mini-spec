# Stop Reviewing AI Code You Don't Understand

For personal projects, I let AI generate entire features. Spec it, generate it, ship it. Works fine—it's my side project, worst case I learn something when it breaks.

Production code is different. When you're pushing to enterprise systems that handle real users and real money, "I think this looks right" isn't good enough. You need to actually understand what's going into production.

That's the tension: engineers want the velocity AI provides, but they also need the confidence to push to production without that nagging feeling that they're shipping code they don't fully grasp.

The current spec-driven workflow doesn't solve this. It optimizes for speed at the cost of understanding.

## The Tourist Problem

I started noticing something troubling. I'd merge a PR generated by AI, and a week later when a bug popped up, I'd stare at the code like a stranger. "Did I write this? Why is this here?" I didn't write it. I reviewed it—allegedly—but reviewing 400 lines of unfamiliar code is fundamentally different from writing it.

You become a tourist in your own codebase. The mental model never forms. Debugging becomes archaeology.

## Review Fatigue is Real

The second issue is more insidious. After AI generates a complete feature, you're faced with a massive PR. Maybe 300 lines, maybe 800. You start reading. Around line 150, your attention starts drifting. By line 300, you're skimming. By line 500, you're rubber-stamping.

This isn't laziness. It's cognitive reality. Reviewing code you didn't write is exhausting. Your brain has no context, no history, no "I chose this approach because..." to lean on.

## Pair Programming Was Always the Answer

The thing is, we solved this problem decades ago. It's called pair programming. One person drives, one navigates. The navigator maintains context, asks questions, catches issues. The driver focuses on implementation.

The insight is obvious in retrospect: AI should be the driver, not a solo contractor who dumps code on your desk.

## MiniSpec: What I Built

I forked SpecKit—a solid spec-driven development toolkit—and rewired it around this idea.

Instead of AI generating artifacts for you to read, every step is a conversation:

**Design becomes a dialogue.** AI asks clarifying questions. Presents options with trade-offs. You make decisions. Those decisions get documented automatically.

**Tasks become bite-sized.** Based on your preference (I like 40-80 lines), AI proposes chunks. You adjust. Nothing gets implemented that you can't review comfortably.

**Implementation becomes pairing.** AI explains what it's about to do. Implements a small chunk. You review, ask questions if something's unclear, then approve. Repeat. Each commit is small enough to understand.

The key shift: you're involved throughout. Not reviewing at the end—navigating in real-time.

## The Documentation Side Effect

Something else happened that I didn't expect. Documentation started writing itself.

When AI has to explain its reasoning to you, and you have to make decisions, those explanations and decisions need to go somewhere. So they go into decision records. Automatically.

When AI implements a pattern and explains why, that pattern gets documented. When you question a choice, the clarification gets captured.

The knowledge base grows as a byproduct of the pairing process. Not as a separate task you never get around to.

## Configurable Autonomy

Not every chunk needs the same level of attention. MiniSpec lets you configure this:

- **Chunk size**: How many lines before AI pauses for review
- **Autonomy triggers**: When can AI proceed without asking (e.g., if tests pass)
- **Doc review**: Trust AI on documentation, or review everything

This isn't about removing human judgment. It's about putting it where it matters.

## What It Actually Looks Like

```text
> /minispec.design "add user authentication"

AI: "Let's design authentication. First: what's your expected user scale?"

> Around 5000 users

AI: "For that scale, both session-based and JWT work. Given your
    Kubernetes setup, JWT is simpler operationally. Trade-offs: [...]
    Preference?"

> JWT

AI: "Documenting in decisions/001-jwt-auth.md. Next: token storage..."
```

The conversation continues through each decision. Then tasks get broken down. Then implementation happens in reviewable chunks.

At every step, I understand what's happening and why. The mental model forms through the dialogue.

## Is It Slower?

Honestly, yes—for the first few features. You're actively engaged instead of waiting for AI to finish. That takes energy.

But here's the thing: I'm not debugging code I don't understand anymore. I'm not re-learning my own codebase every time I return to it. The time investment front-loads understanding instead of back-loading confusion.

And the documentation that accumulates along the way? That's pure time savings when someone else (or future-me) needs to understand the system.

## Try It

MiniSpec is open source, built on SpecKit's infrastructure. If you're already using AI coding tools and feeling the review fatigue, give it a shot.

```bash
uv tool install minispec-cli --from git+https://github.com/ivo-toby/mini-spec.git
minispec init my-project --ai claude
```

The repo has all the commands documented. Start with `/minispec.constitution` to set your preferences, then run through a feature with `/minispec.design` and `/minispec.next`.

I'd be curious what you find. The workflow assumes certain things about how people work with AI—I'm probably wrong about some of them.

---

*MiniSpec is a fork of [SpecKit](https://github.com/github/spec-kit). Thanks to Den Delimarsky and John Lam for the foundation.*
